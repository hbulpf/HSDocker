# Minikube 操作记录  

## 1.关于上次的CrashLoopBackoff的解决:  
关于CrashLoopBackoff的解决方案:  
在stack overflow上找到解答:  
**The issue caused by the docker container which exits as soon as the "start" process finishes. Add a command that runs forever and probably it'll work. For example, recreating docker image**:  
在Dockerfile上在底层hadoop镜像的基础上加个:  
```
CMD exec /bin/bash -c "trap : TERM INT; sleep infinity & wait"
```  
原镜像的最后一行是通过CMD启动ssh service,原因应该是启动完后pod就自动结束了，然后再重复启动，结束造成死循环。解决办法就是加个命令让容器始终在运作。  

## 2.成功启动pod后
之后Pod可以成功运行起来，开了个master节点的pod和两个slave节点的pod。  
hadoop-master.yaml:  
```
apiVersion: v1
kind: Pod
metadata:
  name: hadoop-master
  labels:
    app: hadoop-master
spec:
  containers:
    - name: hadoop-master
      image: chellyk-spark2
      imagePullPolicy: Never
```  
hadoop-slave.yaml:  
```
apiVersion: v1
kind: Pod
metadata:
  name: hadoop-slave1
  labels:
    app: hadoop-slave
spec:
  containers:
    - name: hadoop-slave1
      image: chellyk-spark2
      imagePullPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  name: hadoop-slave2
  labels:
    app: hadoop-slave
spec:
  containers:
    - name: hadoop-slave2
      image: chellyk-spark2
      imagePullPolicy: Never
```  

相对于原先用脚本启动hadoop集群，少了bridge网桥，只能手动添加ip 跟主机名的映射到各个节点的/etc/hosts文件. 在启动过程中发现ssh无法登陆，最后发现是ssh service没启动，原因不明？因为原镜像的最后一行CMD就是启动ssh service。  

手动在三个节点service ssh start，修改/etc/hosts文件后，可以正常启动集群，正常使用wordcount单词计数。  

master节点ip是172.17.0.5  
浏览器通过172.17.0.5:8088 , 172.17.0.5:50070可以正常通过web查看集群情况
(在写yaml时我故意没有加端口映射，但却能正常访问这些端口？很奇怪)  

## 3.稍作改进的yaml  
hadooop-master.yaml:  
```
apiVersion: v1
kind: Service
metadata:
  name: hadoop-nn-service
  labels:
    app: hadoop-nn
spec:
  ports:
    - port: 9000
      name: hdfs
    - port: 50070
      name: name-node
  clusterIP: None
  selector:
    app: hadoop-master
---
apiVersion: v1
kind: Pod
metadata:
  name: hadoop-master
  labels:
    app: hadoop-master
spec:
  containers:
    - name: hadoop-master
      image: chellyk-spark2
      imagePullPolicy: Never
```  

hadoop-slave.yaml: 
```
apiVersion: v1
kind: Service
metadata:
  name: hadoop-dn-service
  labels:
    app: hadoop-dn
spec:
  ports:
    - port: 9000
      name: hdfs
    - port: 50010
      name: data-node-trans
    - port: 50075
      name: data-node-http
  clusterIP: None
  selector:
    app: hadoop-slave
---
apiVersion: v1
kind: Pod
metadata:
  name: hadoop-slave1
  labels:
    app: hadoop-slave
spec:
  containers:
    - name: hadoop-slave1
      image: chellyk-spark2
      imagePullPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  name: hadoop-slave2
  labels:
    app: hadoop-slave
spec:
  containers:
    - name: hadoop-slave2
      image: chellyk-spark2
      imagePullPolicy: Never
```  

hadoop-web.yaml:  
```
apiVersion: v1
kind: Service
metadata:
  name: hadoop-ui-service
  labels:
    app: hadoop-nn
spec:
  ports:
    - port: 8088
      name: resource-manager
    - port: 50070
      name: name-node
  selector:
    app: hadoop-nn
  type: NodePort
```  

## 4.测试statefulSet:  
```
apiVersion: v1
kind: Service
metadata:
  name: hadoop-dn-service
  labels:
    app: hadoop-dn
spec:
  ports:
    - port: 9000
      name: hdfs
    - port: 50010
      name: data-node-trans
    - port: 50075
      name: data-node-http
  clusterIP: None
  selector:
    app: hadoop-slave
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: hadoop-slave
spec:
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: hadoop-slave
  serviceName: hadoop-dn-service
  template:
    metadata:
      labels:
        app: hadoop-slave
    spec:
      containers:
        - name: hadoop-slave
          image: chellyk-spark2
          imagePullPolicy: Never
```  

指定起两个slave节点的pod。
metadata将name设置成hadoop-slave，生成两个容器的主机名有规律:  
hadoop-slave-0  
hadoop-slave-1  

经测试可以正常使用 

## 5.还存在的问题  
1.每次启动集群都需要在/etc/hosts文件填各节点ip 主机名的映射。
2.因为这只是一个3节点的hadoop集群。如果是要建立大数据实验平台，肯定是要建立多个3节点的hadoop集群，如何将各个集群隔离开，怎么区分？

## 6.关于YAML里，spec.selector的作用  
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: hadoop-slave
spec:
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: hadoop-slave
  serviceName: hadoop-dn-service
  template:
    metadata:
      labels:
        app: hadoop-slave
    spec:
      containers:
        - name: hadoop-slave
          image: chellyk-spark2
          imagePullPolicy: Never
```  
在StatefulSet里，有个selector.matchLabels，这个经实验和文档查阅是不可省略的。如果省略会导致无法创建StatefulSets。  
省略后报错提示:  
```
error validating "hadoop-slave.yaml": error validating data: ValidationError(StatefulSet.spec): missing required field "selector" in io.k8s.api.apps.v1.StatefulSetSpec; if you choose to ignore these errors, turn validation off with --validate=false
```

官方文档里有详细解释:  
**You must set the .spec.selector field of a StatefulSet to match the labels of its .spec.template.metadata.labels**.
Prior to Kubernetes 1.8, the .spec.selector field was defaulted when omitted. 
In 1.8 and later versions, failing to specify a matching Pod Selector will result in a validation error during StatefulSet creation

## 7.spark启动问题
Spark的启动与hadoop基本一样，都依赖SSH，所以启动spark还是要先启动各节点的ssh service,修改/etc/hosts文件，同时修改/usr/local/spark/conf/slaves文件。  
目前测试了简单的wordcount，因为spark需要比较多内存，目前各节点的内存不足以支持使用spark。  

## 8.关于service的使用问题  
新增了hadoop-web.yaml,通过Nodeport的方式，暴露Node节点ip的端口，目的是希望外部能够访问hadoop，spark集群的Web界面:  
```
apiVersion: v1
kind: Service
metadata:
  name: hadoop-ui-service
spec:
  ports:
    - port: 8088
      name: resource-manager
    - port: 50070
      name: name-node
    - port: 8080
      name: spark-master
  selector:
    app: hadoop-master
  type: NodePort
```   

创建hadoop-ui-service后:
```
[root@node01 yaml]# kubectl get service
NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                         AGE
hadoop-dn-service   ClusterIP   None             <none>        9000/TCP,50010/TCP,50075/TCP                    4d
hadoop-nn-service   ClusterIP   None             <none>        9000/TCP,50070/TCP                              4d
hadoop-ui-service   NodePort    170.170.35.123   <none>        8088:11607/TCP,50070:37204/TCP,8080:26250/TCP   8s
kubernetes          ClusterIP   170.170.0.1      <none>        443/TCP                                         11d
```

设想的效果是，例如这里master节点的pod在Node03，我们希望的是通过访问node03的ip:11607这种形势去访问hadoop集群的yarn界面，但目前测试还不行。因为Node的IP是虚拟机内网？这方面我不太清楚。








