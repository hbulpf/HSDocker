# 实验四十 综合实战：环境大数据

## 40.1 实验目的  
1．学会分析环境数据文件；
2．学会编写解析环境数据文件并进行统计的代码；
3．学会进行递归MapReduce。  

## 40.2 实验要求  
要求实验结束时，每位学生均已在服务器上运行从广州2017年1月到6月这半年间的空气质量数据文件中分析出的环境统计结果，包含每日的平均PM2.5浓度、半年的总体空气质量情况等。  

## 40.3 实验原理  
近年来，由于雾霾问题的持续发酵，越来越多的人开始关注城市相关的环境数据，包括空气质量数据、天气数据等等。  
如果每小时记录一次城市的天气实况和空气质量实况信息，则每个城市每天都会产生24条环境数据，全国所有2500多个城市如果均如此进行记录，那每天产生的数据量将达到6万多条，每年则会产生2190万条记录，已经可以称得上环境大数据。  
对于这些原始监测数据，我们可以根据时间的维度来进行统计，从而得出与该城市相关的日度及月度平均气温、空气质量优良及污染天数等等，从而为研究空气污染物扩散条件提供有力的数据支持。  
本实验中选取了广州2017年1月到6月这半年间的每小时的空气质量数据（原数据中“Missing”表示缺失数据)，利用MapReduce来统计半年内空气质量为优、良、轻度污染、中度污染、重度污染和严重污染的天数。  

## 40.4 实验步骤  

### 40.4.1 获取数据文件  
原数据文件下载链接: !下载链接)[] 

这次实验我们剔除了部分不需要的信息，最后整理成如下格式(以下为部分数据):  
数据内容分别对应:**城市/类别/日期/时间/浓度/数据是否有效(Missing表示数据缺失)**
```
Guangzhou PM2.5 1/3/2017 0:00 -999 Missing
Guangzhou PM2.5 1/3/2017 1:00 -999 Missing
Guangzhou PM2.5 1/3/2017 2:00 -999 Missing
Guangzhou PM2.5 1/3/2017 3:00 -999 Missing
Guangzhou PM2.5 1/3/2017 4:00 -999 Missing
Guangzhou PM2.5 1/3/2017 5:00 -999 Missing
Guangzhou PM2.5 1/3/2017 6:00 -999 Missing
Guangzhou PM2.5 1/3/2017 7:00 -999 Missing
Guangzhou PM2.5 1/3/2017 8:00 -999 Missing
Guangzhou PM2.5 1/3/2017 9:00 -999 Missing
Guangzhou PM2.5 1/3/2017 10:00 -999 Missing
Guangzhou PM2.5 1/3/2017 11:00 61 Valid
Guangzhou PM2.5 1/3/2017 12:00 39 Valid
Guangzhou PM2.5 1/3/2017 13:00 38 Valid
Guangzhou PM2.5 1/3/2017 14:00 37 Valid
Guangzhou PM2.5 1/3/2017 15:00 59 Valid
Guangzhou PM2.5 1/3/2017 16:00 58 Valid
Guangzhou PM2.5 1/3/2017 17:00 77 Valid
Guangzhou PM2.5 1/3/2017 18:00 88 Valid
Guangzhou PM2.5 1/3/2017 19:00 150 Valid
Guangzhou PM2.5 1/3/2017 20:00 162 Valid
Guangzhou PM2.5 1/3/2017 21:00 141 Valid
Guangzhou PM2.5 1/3/2017 22:00 106 Valid
Guangzhou PM2.5 1/3/2017 23:00 78 Valid
```  

将数据文件上传至HDFS:  
```
root@master:~# hadoop fs -put data.txt /
root@master:~# hadoop fs -ls /          
Found 1 items
-rw-r--r--   2 root supergroup     180941 2018-11-02 07:58 /data.txt
```  

### 40.4.2 编写每日PM2.5浓度统计程序  
打开IDEA，首先编写常规的mapreduce任务需要导入两个jar包:  
```
hadoop-common-2.7.7.jar  
hadoop-mapreduce-client-core-2.7.7.jar
```  
具体位置在哪这里不多赘述。  

创建项目Environment_data项目, map阶段只会将"Valid"数据进行处理。
```  
import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;

public class DailyAverage {
    public static class StatMapper extends Mapper<Object, Text, Text, IntWritable>{
        public void map(Object key, Text value, Context context) throws IOException, InterruptedException{
            String[] items = value.toString().split(" ");

            String valid = items[5];
            if(valid.equals("Valid")){
                String date =items[2];
                String PM_value = items[4];
                context.write(new Text(date),new IntWritable(Integer.parseInt(PM_value)));
            }
        }
    }

    public static class StatReducer extends Reducer<Text, IntWritable, Text, DoubleWritable>{
        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException{
            int count = 0;
            double PM_average = 0;

            for(IntWritable value: values){
                PM_average += value.get();
                count++;
            }

            PM_average /= count;
            context.write(key,new DoubleWritable(PM_average));
        }
    }

    public static void main(String args[])  throws IOException, ClassNotFoundException, InterruptedException{

        Configuration conf = new Configuration();
        Job job = new Job(conf, "PM2.5Daily");
        job.setInputFormatClass(TextInputFormat.class);
        TextInputFormat.setInputPaths(job, args[0]);
        job.setJarByClass(DailyAverage.class);
        job.setMapperClass(StatMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);
        job.setReducerClass(StatReducer.class);
        job.setNumReduceTasks(Integer.parseInt(args[2]));
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(DoubleWritable.class);
        job.setOutputFormatClass(TextOutputFormat.class);
        TextOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);

    }
}
```  

打包成jar包上传至master节点，运行jar包:
```
root@master:~# hadoop jar Environment_data.jar /input /output 3  
```  
3为Reducer数量  

查看结果:  
![image](./images/ex40/Screenshot%20from%202018-10-30%2020-03-56.png)   

![image](./images/ex40/Screenshot%20from%202018-10-30%2020-04-04.png)  

![image](./images/ex40/Screenshot%20from%202018-10-30%2020-04-11.png)  

3个Reducer即将结果分成了三个文件。  

### 40.4.3 编写污染天数统计程序  
在上个mapduce中我们计算了半年来广州每天的平均PM2.5浓度，接下来我们根据空气质量评定标准对半年来的空气质量情况进行统计。  

首先，将三个part结果文件里的数据整合在PMdaily.txt文件中:  
![image](./images/ex40/Screenshot%20from%202018-10-30%2020-04-29.png)

最后一行显示数据总条数为172，为什么不足半年的天数呢？因为原数据有些天24小时的数据全部都是"Missing"无法使用。  

将数据文件上传至HDFS:  
```
root@master:~# hadoop fs -put PMdaily.txt /
root@master:~# hadoop fs -ls /             
Found 4 items
-rw-r--r--   2 root supergroup       4283 2018-11-02 08:12 /PMdaily.txt
-rw-r--r--   2 root supergroup     180941 2018-11-02 07:58 /data.txt
drwxr-xr-x   - root supergroup          0 2018-11-02 08:03 /output
```

以下是mapreduce代码,创建项目Environment_cal:  
```
import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;

public class Environment_cal {
    public static class StatMapper extends Mapper<Object, Text, Text, IntWritable>{
        public void map(Object key, Text value, Context context) throws IOException, InterruptedException{
            String[] items = value.toString().split("\t");
            double PM_average = Double.parseDouble(items[1]);
            String level ="";

            if(PM_average <= 35 )
                level = "优";
            else if(PM_average <= 75)
                level = "良";
            else if(PM_average <= 115)
                level = "轻度污染";
            else if(PM_average <= 150)
                level = "中度污染";
            else if(PM_average <= 250)
                level = "重度污染";
            else
                level = "严重污染";

            context.write(new Text(level),new IntWritable(1));
        }
    }

    public static class StatReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
        public void reduce(Text key, Iterable<IntWritable> values,Context context)throws IOException, InterruptedException{
            int sum = 0;
            for (IntWritable val : values)
                sum += val.get();
            context.write(key, new IntWritable(sum));
        }
    }

    public static void main(String args[]) throws IOException, ClassNotFoundException, InterruptedException{
        Configuration conf = new Configuration();
        Job job = new Job(conf, "AqiStat");
        job.setInputFormatClass(TextInputFormat.class);
        TextInputFormat.setInputPaths(job, args[0]);
        job.setJarByClass(Environment_cal.class);
        job.setMapperClass(StatMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);
        job.setReducerClass(StatReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        job.setOutputFormatClass(TextOutputFormat.class);
        TextOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```  

打包成jar包，上传至master节点并运行:  
```
root@master:~# hadoop jar Environment_cal.jar /PMdaily.txt /output1
```

查看结果:  
![image](./images/ex40/Screenshot%20from%202018-10-30%2020-04-38.png)  

至此实验结束，可见广州的空气质量还是比较好的。  


